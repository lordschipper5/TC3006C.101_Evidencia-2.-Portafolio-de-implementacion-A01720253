{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Modelo Random Forest Classifier para el dataset de Titanic\n",
        "\n",
        "Dentro de este codigo se incorpora una solucion para crear un modelo tipo Random Forest para predecir si un pasajero sobrevive o no. A continuacion se explica el proceso por medio de comentarios y estrucutra de codigo para eficientar el aprendizaje del modelo y sus caracterisicas. La razon por la que se escogio este dataset fue por el motivo de que ya tenemos el ETL para analizar los datos de forma mas organizada y asi llevar a tener mejores resultados. Los datos trnsofmrados se encuentran el archivo train_data y train_results. Usaremos esta informacion para alimentar el modelo y finalmente usaremos tests para hacer pruebas.\n",
        "\n",
        "La referencia para hacer este modelo se compoarte en la siguiente liga: https://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
        "\n",
        "Liga del dataset (No incluye el proceso de ETL): https://www.kaggle.com/competitions/titanic\n",
        "\n",
        "Nombre: Rodolfo Sandoval\n",
        "Matricula: A01720253\n",
        "\n",
        "Nota: En caso de correr el codigo localmente asegurarse de tener el ruteo y la direccion correta de los archivos **train_data.csv** donde estan las caracteristicas y **split_survived.csv** donde esta el valor objetivo.\n",
        "\n",
        "Descripcion de los datos:\n",
        "\n",
        "Registros: 891\n",
        "\n",
        "Numero de Caracteristicas: 6\n",
        "Pclass, Age, Sex, Fam \"Cantidad de familia\", Fare, y Embarked.\n",
        "\n",
        "Clases: 1 \"Sobrevivio\", 0 \"No sobrevivio\"\n",
        "\n",
        "Se utilizaron estas caracteristicas ya que cada una influye y tiene un peso con de acuerdo a la correlacion con la clase de survived."
      ],
      "metadata": {
        "id": "86PY7J8lPUKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a estar usando varios decision trees. El objetivo es combinarlos y agregar las predicciones para tomar una decision final."
      ],
      "metadata": {
        "id": "Xef4cfHXR9M_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOiUo_j1PDxX",
        "outputId": "985b29e3-2a8f-4978-df11-0edbcbdc155e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PassengerID  Prediccion  Etiqueta Actual\n",
            "0              8           0                0\n",
            "1             16           1                1\n",
            "2             17           0                0\n",
            "3             23           1                1\n",
            "4             24           0                1\n",
            "..           ...         ...              ...\n",
            "174          866           1                1\n",
            "175          870           1                1\n",
            "176          871           0                0\n",
            "177          875           1                1\n",
            "178          884           0                0\n",
            "\n",
            "[179 rows x 3 columns]\n",
            "Precision del modelo: 0.8324022346368715\n",
            "Archivo CSV 'model_results.csv' guardado exitosamente.\n",
            "Precisión: 0.8205128205128205\n",
            "Exhaustividad (Recall): 0.8\n",
            "F1-score: 0.810126582278481\n",
            "Matriz de Confusión:\n",
            "[[85 14]\n",
            " [16 64]]\n",
            "Exactitud (Accuracy): 0.8324022346368715\n"
          ]
        }
      ],
      "source": [
        "#Importacion de librerias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#No es para el modelo sino para evaluarlo con metricas\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "# Funcion para dividir los datos en conjuntos de entrenamiento y prueba\n",
        "def train_test_split(data, proporcion=0.2):\n",
        "    np.random.shuffle(data)  #Mezaclamos los datos (data) con np.random.shuffle\n",
        "    split_indice = int(len(data) * (1 - proporcion)) #Calculamos el indice en donde se dividen los datos de la proporcion (proporcion)\n",
        "    train_data = data[:split_indice] #Datos desde el inicio hasta donde se hace la division\n",
        "    test_data = data[split_indice:] #Datos desde la division hasta el final\n",
        "    return train_data, test_data #Devolvemos el conjunto de los datos divididos\n",
        "\n",
        "# Funcion para calcular la impureza Gini\n",
        "def gini_impureza(etiquetas):\n",
        "    clases_unicas, counts = np.unique(etiquetas, return_counts=True) #Encontramos las clases unicas\n",
        "    clases_prob = counts / len(etiquetas) #Calculamos las probabilidades de las clases en relacion a la longitud total de etiquetas\n",
        "    gini = 1 - np.sum(clases_prob ** 2) #Calculamos la impuresa de Gini (con esto determinamos como dividir los datos en los nodos del arbol)\n",
        "    return gini\n",
        "\n",
        "# Funcion para dividir los datos en funcion de un valor de umbral data = datos, feature_idx = indice donde se basa la division, y umbral = el valor umbral que se realiza para la division\n",
        "def split_data(data, feature_idx, umbral):\n",
        "    izq = data[data[:, feature_idx] <= umbral] #subconjunto con los valores sean menor o iguales al umbral\n",
        "    derecha = data[data[:, feature_idx] > umbral] #subconjunto cuando sean mayores al umbral\n",
        "    return izq, derecha\n",
        "\n",
        "# Funcion para encontrar la mejor division para un conjunto de datos\n",
        "def find_best_split(data):\n",
        "    best_gini = float(\"inf\") # Inicializar con un valor alto para comparacion\n",
        "    best_feature_idx = None\n",
        "    best_umbral = None\n",
        "\n",
        "    num_features = data.shape[1] - 1 #Restamos la columna que incluye etiquetas\n",
        "\n",
        "#Iteramos sobre las caracteristicas\n",
        "    for feature_idx in range(num_features):\n",
        "        unique_values = np.unique(data[:, feature_idx])\n",
        "        for umbral in unique_values: #Sobre valores unicos\n",
        "            izq, derecha = split_data(data, feature_idx, umbral)\n",
        "            if len(izq) > 0 and len(derecha) > 0: # Dividir los datos en dos subconjuntos: izquierdo y derecho\n",
        "                gini_izq = gini_impureza(izq[:, -1]) #Calcular impureza gini para lado izquierdo\n",
        "                gini_derecha = gini_impureza(derecha[:, -1]) #Calcular impureza gini para lado derecho\n",
        "                weighted_gini = (len(izq) / len(data)) * gini_izq + (len(derecha) / len(data)) * gini_derecha #Caclular la impureza despues de la division\n",
        "\n",
        "                if weighted_gini < best_gini: #Checar si se encuentra una mejor division y replazar esos valores\n",
        "                    best_gini = weighted_gini\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_umbral = umbral\n",
        "\n",
        "    return best_feature_idx, best_umbral\n",
        "\n",
        "# Funcion para construir un arbol de decision\n",
        "def build_tree(data, max_depth):\n",
        "    if max_depth == 0 or len(np.unique(data[:, -1])) == 1: #Checar si max_depth (profundidad de los nodos) es 0 o sin las etiquetas son iguales con unique\n",
        "        clases_unicas, counts = np.unique(data[:, -1], return_counts=True)\n",
        "        return clases_unicas[np.argmax(counts)]\n",
        "\n",
        "    best_feature_idx, best_umbral = find_best_split(data)\n",
        "\n",
        "    # Sacar la clase mayoritaria en caso de que no sea posible hacer una division (Clase con mayor cantidad de ejemplos)\n",
        "    if best_feature_idx is None:\n",
        "        clases_unicas, counts = np.unique(data[:, -1], return_counts=True)\n",
        "        return clases_unicas[np.argmax(counts)]\n",
        "\n",
        "    # Dividir los datos en dos subconjuntos usando la mejor caracteristica y umbral\n",
        "    izq, derecha = split_data(data, best_feature_idx, best_umbral)\n",
        "    izq_subtree = build_tree(izq, max_depth - 1)\n",
        "    derecha_subtree = build_tree(derecha, max_depth - 1)\n",
        "    return (best_feature_idx, best_umbral, izq_subtree, derecha_subtree) #Retornamos un nodo arbol con la mejor caracteristica\n",
        "\n",
        "# Funcion para construir un Random Forest\n",
        "def build_random_forest(data, num_arboles, max_depth):\n",
        "    forest = [] #Lista de los arboles del bosque\n",
        "    for _ in range(num_arboles):\n",
        "        subset = np.random.choice(len(data), size=len(data), replace=True) #Creamos subconjunto aleatorio con replace para el conjunto de datos\n",
        "        subset_data = data[subset]\n",
        "        tree = build_tree(subset_data, max_depth) #Contruimos un arbol aleatorio\n",
        "        forest.append(tree) #Agregamos el arbol al bosque\n",
        "    return forest\n",
        "\n",
        "# Funcion para hacer predicciones usando un arbol de decision\n",
        "def predict(tree, sample):\n",
        "    if isinstance(tree, np.int64):  # Si el nodo actual es una clase, devuelve la clase predicha\n",
        "        return tree\n",
        "\n",
        "    feature_idx, umbral, izq_subtree, derecha_subtree = tree\n",
        "\n",
        "    if sample[feature_idx] <= umbral: # Comparamos el valor de la caracteristica (feature_idx) de la muestra con el umbral\n",
        "        return predict(izq_subtree, sample) #Si es menor o igual al umbral nos vamos por el subarbol izq\n",
        "    else:\n",
        "        return predict(derecha_subtree, sample) #Si mayor al umbral nos vamos por el subarbol derecho\n",
        "\n",
        "# Funcion para hacer predicciones con el Random Forest\n",
        "def predict_random_forest(forest, sample):\n",
        "    predi = [predict(tree, sample) for tree in forest]\n",
        "    return np.bincount(predi).argmax()\n",
        "\n",
        "# Cargar los datos de entrenamiento y resultados desde archivos CSV\n",
        "train_data = pd.read_csv(\"train_data.csv\")\n",
        "train_results = pd.read_csv(\"split_survived.csv\")\n",
        "\n",
        "# Combinar los datos de entrenamiento y resultados en un unico DataFrame\n",
        "combined_data = pd.merge(train_data, train_results, on='PassengerId')\n",
        "\n",
        "# Convertir el DataFrame a un arreglo NumPy para facilitar su manejo\n",
        "data_array = combined_data.values\n",
        "\n",
        "train_data, test_data = train_test_split(data_array)\n",
        "num_arboles = 100\n",
        "max_depth = 3\n",
        "forest = build_random_forest(train_data, num_arboles, max_depth)\n",
        "\n",
        "# Hacer predicciones en datos de prueba\n",
        "test_data = test_data[np.argsort(test_data[:, 0])]  # Ordenar por PassengerID\n",
        "predi = [predict_random_forest(forest, sample) for sample in test_data[:, :-1]]\n",
        "actual_etiquetas = test_data[:, -1]\n",
        "\n",
        "# Crear un DataFrame con los resultados\n",
        "results_df = pd.DataFrame({'PassengerID': test_data[:, 0], 'Prediccion': predi, 'Etiqueta Actual': actual_etiquetas})\n",
        "\n",
        "# Ordenar el DataFrame por PassengerID\n",
        "results_df = results_df.sort_values(by='PassengerID')\n",
        "\n",
        "# Imprimir el DataFrame\n",
        "print(results_df)\n",
        "\n",
        "# Calcular la precision\n",
        "accuracy = np.sum(predi == actual_etiquetas) / len(actual_etiquetas)\n",
        "print(\"Precision del modelo:\", accuracy)\n",
        "\n",
        "#Creamos csv con los resultados del modelo\n",
        "results_df.to_csv('model_results.csv', index=False)\n",
        "print(\"Archivo CSV 'model_results.csv' guardado exitosamente.\")\n",
        "\n",
        "#Metricas\n",
        "# Precisión\n",
        "precision = precision_score(actual_etiquetas, predi)\n",
        "\n",
        "# Exhaustividad (Recall)\n",
        "recall = recall_score(actual_etiquetas, predi)\n",
        "\n",
        "# F1-score\n",
        "f1 = f1_score(actual_etiquetas, predi)\n",
        "\n",
        "# Matriz de confusión\n",
        "confusion = confusion_matrix(actual_etiquetas, predi)\n",
        "\n",
        "# Exactitud (Accuracy)\n",
        "accuracy = accuracy_score(actual_etiquetas, predi)\n",
        "\n",
        "print(\"Precisión:\", precision)\n",
        "print(\"Exhaustividad (Recall):\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Matriz de Confusión:\")\n",
        "print(confusion)\n",
        "print(\"Exactitud (Accuracy):\", accuracy)"
      ]
    }
  ]
}